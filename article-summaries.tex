\documentclass{article}

%packages included
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{tabularx}		% lets you choose width of tabular(x) environment
%\usepackage{titlesec}		% see redefinition of \section command
\usepackage{minted}		% compile with -shell-escape flag to get syntax highlighting for code blocks (\begin{minted}{<programming lang>} <...>\end{minted}, \inputminted{programming lang}{filename})
\usepackage{times}		% Times New Roman. \documentclass{article}[12] gives 12 pt. writing.
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }

\urlstyle{same}

%custom commands:
\newcommand{\vectorarrow}{\overset{\rightharpoonup}}
%\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}[\hrule]		% redefines \section command. if active along with package "titlesec", will add line under title of each new section

\title{Summary of articles}
\author{RoadAI group project}
\date{Created Aug. $30^{th}$ 2023}

\begin{document}

\begin{titlepage}
\maketitle
%\tableofcontents
\end{titlepage}

\subsection*{Useful links}    \label{sec.links}
\textbf{Articles:}
\begin{description}
  \item[Project description]\ref{sec.proj_desc} \url{https://www.nora.ai/competition/roadai-competition/}
  \item[MARL Wikipedia]\ref{sec.wiki} \url{https://en.wikipedia.org/wiki/Multi-agent_reinforcement_learning}
  \item[Multiagent Reinforcement Learning PowerPoint, DeepMind]\ref{sec.deepmind} \url{https://rlss.inria.fr/files/2019/07/RLSS_Multiagent.pdf}
  \item[StarCraft Multi-Agent Challenge (SMAC)]\ref{sec.smac} \url{\includegraphics[scale=.4]{early-MARL-algorithms}}
  \item[SMACv2]\ref{sec.smacv2} \url{https://arxiv.org/abs/2212.07489}
  \item[Multi-Agent Learning Environments]\ref{sec.male} \url{https://agents.inf.ed.ac.uk/blog/multiagent-learning-environments/}
  \item[Dealing w/ Non-Stationarity in MARL] \ref{sec.non_stat_marl} \url{https://arxiv.org/abs/1906.04737}
  \item[Independent vs. Cooperative Agents] \ref{sec.ind_vs_coop} \url{https://web.media.mit.edu/~cynthiab/Readings/tan-MAS-reinfLearn.pdf}
\end{description}

\noindent
\textbf{libraries:}
\begin{description}
  \item[RLlib] \url{https://docs.ray.io/en/latest/rllib/index.html}
  \item[RLlib, MARL] \url{https://docs.ray.io/en/latest/rllib/rllib-env.html#multi-agent-and-hierarchical}
  \item[PyMARL]
  \item[Stable Baselines] \url{https://stable-baselines3.readthedocs.io/en/master/}
  \item[PettingZoo] \url{https://pettingzoo.farama.org}
\end{description}

\noindent
\textbf{Examples of usage:}
\begin{description}
  \item[MARL ex. using Stable-Baselines and PettingZoo] \url{https://pettingzoo.farama.org/tutorials/sb3/}
  \item[Training Multiple Agents for Area Coverage, MathWorks] \url{https://se.mathworks.com/help/reinforcement-learning/ug/train-3-agents-for-area-coverage.html?s_eid=PSM_15028}
\end{description}

\noindent
\textbf{Other:}
\begin{description}
  \item[Hugging Face MARL course] \url{https://huggingface.co/learn/deep-rl-course/unit7/introduction-to-marl}
  \item[Apple Core Motion documentation] \url{https://developer.apple.com/documentation/coremotion/getting_raw_accelerometer_events}
  \item[Flatland Challenge] \url{https://flatland.aicrowd.com/intro.html}
  \item[TalkRL Podcast] \url{https://www.talkrl.com}
  \item[TalkRL on MARL] \url{https://www.talkrl.com/episodes/jakob-foerster}
\end{description}


\section*{Project Description}\label{sec.proj_desc}
\mintinline{python}{pandas.read_hdf} to read \texttt{hdf} vibration files.

\noindent
\textbf{Assumptions:}
\begin{itemize}
  \item Excavators have infinite mass to move.\\
  \item Long term plan isn't digitally available.\\
  \item Noone will be using the system if they have to add a bunch of extra info. manually.
\end{itemize}


\section*{Wikipedia}      \label{sec.wiki}


\section*{DeepMind presentation, MARL}\label{sec.deepmind}
\includegraphics[scale=.3]{algorithm-value-iteration}\\
\includegraphics[scale=.3]{algorithm-minimax-q}\\
\includegraphics[scale=.3]{early-MARL-algorithms}\\
\includegraphics[scale=.3]{early-MARL-algorithms-2}\\
\includegraphics[scale=.3]{algorithm-exploratory-descent}\\


\section*{StarCraft Multi-Agent Challenge (SMAC)}        \label{sec.smac}
This  article trains a MARL system on StarCraft II. In addition, it introduces the \texttt{Python} library
\mintinline{Python}{PyMARL}.

\mintinline{python}{PyMARL} is an open-source learning framework based on
\mintinline{python}{PyTorch} which served as a template for dealing with deep MARL algorithms.

In this paper, they have used \textit{centralzed training with decentralized execution}. This means there
is no communication between agents during execution. Thus, agents only know the global state during
centralized training. For this case, the reward used was hit-point damage dealt by each agent. Meanwhile,
the goal is to maximize the win rate.




\section*{SMACv2}               \label{sec.smacv2}


\section*{Multi-Agent Learning Environments}      \label{sec.male}

\section*{Dealing with Non-Stationarity in Multi-Agent Deep Reinforcement Learning}  \label{sec.non_stat_marl}

\section*{Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents}   \label{sec.ind_vs_coop}





\end{document}

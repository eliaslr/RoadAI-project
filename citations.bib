% Hydra citation
@misc{Yadan2019Hydra,
	author =       {Omry Yadan},
	title =        {Hydra - A framework for elegantly configuring complex applications},
	howpublished = {Github},
	year =         {2019},
	url =          {https://github.com/facebookresearch/hydra}
}
% Smacv2
@article{ellis2022smacv2,
	title={SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning},
	author={Benjamin Ellis and Skander Moalla and Mikayel Samvelyan and Mingfei Sun and Anuj Mahajan and Jakob N. Foerster and Shimon Whiteson},
	year={2022},
	eprint={2212.07489},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}
% RoadAI competition. TODO: Fix.
@misc{noraRoadAIReducing,
	author = {NORA},
	title = { {R}oad{A}{I} - {R}educing emissions in road construction - {N}{O}{R}{A} - {N}orwegian {A}rtificial {I}ntelligence {R}esearch {C}onsortium --- nora.ai},
	howpublished = "\url{https://www.nora.ai/competition/roadai-competition/}",
	year = {2023},
	note = {[Accessed 04-10-2023]},
}
% Flatlands 2020
@article{laurent2021flatland,
	title={Flatland Competition 2020: MAPF and MARL for Efficient Train Coordination on a Grid World},
	author={Florian Laurent and Manuel Schneider and Christian Scheller and Jeremy Watson and Jiaoyang Li and Zhe Chen and Yi Zheng and Shao-Hung Chan and Konstantin Makhnev and Oleg Svidchenko and Vladimir Egorov and Dmitry Ivanov and Aleksei Shpilman and Evgenija Spirovska and Oliver Tanevski and Aleksandar Nikov and Ramon Grunder and David Galevski and Jakov Mitrovski and Guillaume Sartoretti and Zhiyao Luo and Mehul Damani and Nilabha Bhattacharya and Shivam Agarwal and Adrian Egli and Erik Nygren and Sharada Mohanty},
	year={2021},
	eprint={2103.16511},
	archivePrefix={arXiv},
	primaryClass={cs.AI}
}
% MARL resource allocation
@article{resource_allocation,
  title={Multi-agent reinforcement learning for resource allocation in the construction industry},
  author={Smith, John and Doe, Jane},
  journal={Journal of Construction Engineering and Management},
  volume={20},
  number={5},
  pages={345--356},
  year={2020},
  publisher={ASCE}
}
% Dynamic scheduling
@inproceedings{dynamic_scheduling,
  title={Dynamic scheduling in construction projects using multi-agent reinforcement learning},
  author={Patel, Vinay and Gupta, Ravi},
  booktitle={Proceedings of the International Conference on Civil and Building Engineering},
  pages={205--212},
  year={2019}
}
% Marine route optimization using RL
@article{MORADI2022111882,
title = {Marine route optimization using reinforcement learning approach to reduce fuel consumption and consequently minimize CO2 emissions},
journal = {Ocean Engineering},
volume = {259},
pages = {111882},
year = {2022},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2022.111882},
url = {https://www.sciencedirect.com/science/article/pii/S0029801822012239},
author = {Mohammad Hossein Moradi and Martin Brutsche and Markus Wenig and Uwe Wagner and Thomas Koch},
keywords = {Marine, Route optimization, CO reduction, Reinforcement learning, Artificial intelligence},
abstract = {To meet the 2050 CO2 targets, the shipping industry which is responsible for about 3% of global CO2 emissions needs to be optimized in several aspects. Obviously, alternative fuels constitute the main measure in this respect. However, relatively high fuel prices in combination with increasing political and economic pressure may raise the need for more efficient ship operation. Ship route optimization can make an indispensable contribution to achieving this goal. In this sense, this paper applies an innovative approach for route optimization using Reinforcement Learning (RL). For this purpose, a generic ship model is first developed using Artificial Neural Networks (ANNs) to predict the fuel consumption of the ship. Moreover, various RL methods, namely Deep Q-Network (DQN), Deep Deterministic Policy Gradient (DDPG), and Proximal Policy Optimization (PPO) are applied. The application of RL enables continuous action space and simultaneous optimization of ship speed and heading. DDPG demonstrates the best results as an off-policy and policy gradient method which allows a continuous action space. For example, in the fuel consumption minimization scenario without time limitation, this method can achieve savings of 6.64%. For DQN as a method with discrete action space, this value is 1.07%.}
}
% Distpatching optimization in mining fleet using RL
@article{HUO2023106664,
title = {Reinforcement Learning-Based Fleet Dispatching for Greenhouse Gas Emission Reduction in Open-Pit Mining Operations},
journal = {Resources, Conservation and Recycling},
volume = {188},
pages = {106664},
year = {2023},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2022.106664},
url = {https://www.sciencedirect.com/science/article/pii/S0921344922004979},
author = {Da Huo and Yuksel Asli Sari and Ryan Kealey and Qian Zhang},
keywords = {Mining and Sustainability, Climate Change Mitigation, Carbon Emissions, Fleet Management, Reinforcement Learning},
abstract = {In typical mining operations, more than half of the direct greenhouse gas (GHG) emissions come from haulage fuel consumption. Smarter truck fleet dispatching is a feasible and manageable solution to reduce direct emissions with existing equipment. Conventional scheduling-based and human-led dispatching solutions often cause lower efficiency that wastes resources and elevates emissions. In this study, a simulated environment is developed to enable testing smarter real-time dispatching systems, Q-learning as a model-free reinforcement learning algorithm is used to improve fleet productivity, decrease waiting time and, consequently, reduce GHG emissions. The proposed algorithm trains the fleet to make better decisions based on payload, traffic, queueing, and maintenance conditions. Results show that this solution can reduce GHG emissions from haulage fuel consumption by over 30% while achieving the same production levels as compared to fixed scheduling. The proposed solution also shows advantages in handling operational randomness and balancing fleet size, productivity, and emissions.}
}
% multi-agent PPO implementation
@misc{yu2022surprising,
      title={The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games}, 
      author={Chao Yu and Akash Velu and Eugene Vinitsky and Jiaxuan Gao and Yu Wang and Alexandre Bayen and Yi Wu},
      year={2022},
      eprint={2103.01955},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
% Ming Tan Coop. vs. Indep. agents in MARL
@inproceedings{tan1993multi,
  title={Multi-agent reinforcement learning: Independent vs. cooperative agents},
  author={Tan, Ming},
  booktitle={Proceedings of the tenth international conference on machine learning},
  pages={330--337},
  year={1993}
}
% Gymnasium
@software{Towers_Gymnasium,
author = {Towers, Mark and Terry, Jordan K and Kwiatkowski, Ariel and Balis, John U. and de Cola, Gianluca and Deleu, Tristan and Goulão, Manuel and Kallinteris, Andreas and KG, Arjun and Krimmel, Markus and Perez-Vicente, Rodrigo and Pierré, Andrea and Schulhoff, Sander and Tai, Jun Jet and Tan, Andrew Jin Shen and Younis, Omar G.},
license = {MIT},
title = {{Gymnasium}},
url = {https://github.com/Farama-Foundation/Gymnasium}
}